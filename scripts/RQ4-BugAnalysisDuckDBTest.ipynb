{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.chdir('../')\n",
    "SCRIPDIR = os.path.dirname(os.path.abspath(\"test.ipynb\"))\n",
    "sys.path.append(os.path.dirname(SCRIPDIR))\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_a_sample(check_df: pd.DataFrame, result_df: pd.DataFrame = None, idx:int = None):\n",
    "    if check_df[check_df['ERROR_REASON'].isna()].shape[0] == 0:\n",
    "        print(\"No error found!\")\n",
    "        return None\n",
    "    if idx is not None:\n",
    "        sample = check_df.loc[[idx]]\n",
    "    else:\n",
    "        sample = check_df[check_df['ERROR_REASON'].isna() & check_df['IS_ERROR'] == True].sample()\n",
    "    # if result_log is not None:\n",
    "    print(\"-----------I have the schema generated as follows:---------\")\n",
    "        # logs = result_log.iloc[sample.LOGS_INDEX.values[0]]\n",
    "    temp_df = result_df[result_df['TESTFILE_PATH'] == sample['TESTFILE_PATH'].values[0]]\n",
    "    print(sample.index)\n",
    "    for sql in temp_df[(temp_df['CASE_TYPE'] == 'Statement') | (temp_df.index == sample.index.values[0])]['SQL']:\n",
    "        print(sql + ';')\n",
    "        # print(logs.values[0])\n",
    "    print(\"-----------The SQL commands is:---------\")\n",
    "    print(sample.SQL.values[0])\n",
    "    print(\"-----------The expected result is:---------\")\n",
    "    print(sample.EXPECTED_RESULT.values[0])\n",
    "    print(\"-----------The actual result is:---------\")\n",
    "    print(sample.ACTUAL_RESULT.values[0])\n",
    "    print(\"-----------The error message is:---------\")\n",
    "    print(sample.ERROR_MSG.values[0])\n",
    "\n",
    "    print(\"-----------The test file is:---------\")\n",
    "    print(sample.TESTFILE_PATH.values[0])\n",
    "    print(\"___________The index is:___________\")\n",
    "    print(sample.index.values[0])\n",
    "    return sample.index.values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbms_name = 'sqlite'\n",
    "\n",
    "output_path = f\"output/{dbms_name}_duckdb_sample_100.csv\"\n",
    "output_reason_path = f\"output/{dbms_name}_duckdb_sample_100_reason.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open a csv file and read the content\n",
    "df = pd.read_csv(f\"output/{dbms_name}_duckdb_filter_results.csv\")\n",
    "result_log = pd.read_csv(f\"output/{dbms_name}_duckdb_filter_logs.csv\")\n",
    "error_percentage = df[df['IS_ERROR'] == True].shape[0] / df.shape[0]\n",
    "print(1 - error_percentage)\n",
    "error_queries = df[(df['CASE_TYPE'] == 'Query') & (df['IS_ERROR'] == True)].shape[0]\n",
    "error_statements = df[(df['CASE_TYPE'] == 'Statement')& (df['IS_ERROR'] == True)].shape[0]\n",
    "wrong_queries = df[(df['CASE_TYPE'] == 'Query') & (df['ERROR_MSG'] == 'Result MisMatch')].shape[0]\n",
    "print(f\"Queries: {error_queries}, Statements: {error_statements}\")\n",
    "print(f\"Wrong Queries: {wrong_queries}\")\n",
    "\n",
    "# add empty column ERROR_REASON to df\n",
    "df['ERROR_REASON'] = None\n",
    "check_df = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pick_a_sample(check_df, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import utils\n",
    "from copy import copy\n",
    "import re\n",
    "\n",
    "TEMP_FILTER = {\n",
    "    'TYPE_CAST': lambda x: re.match(r'unrecognized token: \\\":\\\"', str(x['ERROR_MSG'])) is not None,\n",
    "    'RANGE': lambda x: re.search(r'range\\((.*?)\\)', str(x['SQL']), re.DOTALL | re.IGNORECASE) is not None,\n",
    "    \"GENERAL\": lambda x: re.search(r'no such function: .*', str(x['ERROR_MSG']), re.IGNORECASE) is not None,\n",
    "    \"EXPLAIN\": lambda x: re.match(r'EXPLAIN', str(x['SQL'])) is not None,\n",
    "    \"SYNTAX\": lambda x: re.search(r'syntax error', str(x['ERROR_MSG']), re.IGNORECASE) is not None,\n",
    "}\n",
    "\n",
    "new_df = copy(df)\n",
    "reasons = pd.read_csv(\"data/duckdb_suite_errors.csv\")\n",
    "tags = reasons[reasons['DBMS'] == dbms_name]['TAG'].values.tolist()\n",
    "for tag in tags:\n",
    "    if tag not in TEMP_FILTER:\n",
    "        print(f\"Tag {tag} is not in the test filter, implement it first!\")\n",
    "        continue\n",
    "    new_df.loc[new_df.apply(TEMP_FILTER[tag], axis=1) & new_df['IS_ERROR'] == True, 'ERROR_REASON'] = tag\n",
    "    print(tag)\n",
    "    print(reasons[reasons['TAG']==tag]['DESCRIPTION'].values[0])\n",
    "    # print(new_df[new_df['ERROR_REASON']==tag].info())\n",
    "    print(new_df[new_df['ERROR_REASON']==tag].shape[0])\n",
    "\n",
    "check_df = copy(new_df[new_df['IS_ERROR'] == True & new_df['ERROR_REASON'].isna()])\n",
    "print(\"Result MisMatch: \", check_df[check_df['ERROR_MSG'] == 'Result MisMatch'].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = new_df[new_df['IS_ERROR'] == True]\n",
    "sample_100 = errors.sample(n=100, random_state=233)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_100.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = sample_100.iloc[cnt]\n",
    "\n",
    "print(\"-----------I have the schema generated as follows:---------\")\n",
    "temp_df = new_df[new_df['TESTFILE_PATH'] == sample['TESTFILE_PATH']]\n",
    "for sql in temp_df[temp_df['CASE_TYPE'] == 'Statement']['SQL']:\n",
    "    print(sql + ';')\n",
    "print(\"-----------The SQL commands is:---------\")\n",
    "print(sample.SQL)\n",
    "print(\"-----------The expected result is:---------\")\n",
    "print(sample.EXPECTED_RESULT)\n",
    "print(\"-----------The actual result is:---------\")\n",
    "print(sample.ACTUAL_RESULT)\n",
    "print(\"-----------The error message is:---------\")\n",
    "print(sample.ERROR_MSG)\n",
    "print(sample.LOGS_INDEX)\n",
    "cnt += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample['TESTFILE_PATH'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_100_reason = pd.read_csv(output_path)\n",
    "visible_reasons = sample_100_reason[[\n",
    "    'SQL', 'ERROR_REASON', 'ERROR_MSG', 'ACTUAL_RESULT', 'EXPECTED_RESULT',]]\n",
    "reasons_mapping = pd.read_csv('data/duckdb_suite_errors.csv')\n",
    "reasons_mapping = reasons_mapping[reasons_mapping['DBMS'] == dbms_name]\n",
    "# add columns to sample_100_reason according to reasons_mapping\n",
    "visible_reasons = pd.merge(visible_reasons, reasons_mapping,\n",
    "                           right_on='TAG', left_on='ERROR_REASON', how='inner')\n",
    "# visible_reasons.drop(columns=['TAG'], inplace=True)\n",
    "visible_reasons.to_csv(output_reason_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reason_summary = visible_reasons[['SUPER_TAG',  'DESCRIPTION']].groupby(['SUPER_TAG']).count().style.to_latex()\n",
    "print(reason_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbms_name = 'postgresql'\n",
    "\n",
    "output_path = f\"output/{dbms_name}_duckdb_sample_100.csv\"\n",
    "output_reason_path = f\"output/{dbms_name}_duckdb_sample_100_reason.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4934779492570118\n",
      "Queries: 6411, Statements: 4190\n",
      "Wrong Queries: 735\n"
     ]
    }
   ],
   "source": [
    "# open a csv file and read the content\n",
    "df = pd.read_csv(f\"output/{dbms_name}_duckdb_filter_results.csv\")\n",
    "result_log = pd.read_csv(f\"output/{dbms_name}_duckdb_filter_logs.csv\")\n",
    "error_percentage = df[df['IS_ERROR'] == True].shape[0] / df.shape[0]\n",
    "print(1 - error_percentage)\n",
    "error_queries = df[(df['CASE_TYPE'] == 'Query') & (df['IS_ERROR'] == True)].shape[0]\n",
    "error_statements = df[(df['CASE_TYPE'] == 'Statement')& (df['IS_ERROR'] == True)].shape[0]\n",
    "wrong_queries = df[(df['CASE_TYPE'] == 'Query') & (df['ERROR_MSG'] == 'Result MisMatch')].shape[0]\n",
    "print(f\"Queries: {error_queries}, Statements: {error_statements}\")\n",
    "print(f\"Wrong Queries: {wrong_queries}\")\n",
    "\n",
    "# add empty column ERROR_REASON to df\n",
    "df['ERROR_REASON'] = None\n",
    "check_df = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pick_a_sample(check_df, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import utils\n",
    "from copy import copy\n",
    "import re\n",
    "\n",
    "TEMP_FILTER = {}\n",
    "\n",
    "new_df = copy(df)\n",
    "reasons = pd.read_csv(\"data/duckdb_suite_errors.csv\")\n",
    "tags = reasons[reasons['DBMS'] == dbms_name]['TAG'].values.tolist()\n",
    "for tag in tags:\n",
    "    if tag not in TEMP_FILTER:\n",
    "        print(f\"Tag {tag} is not in the test filter, implement it first!\")\n",
    "        continue\n",
    "    new_df.loc[new_df.apply(TEMP_FILTER[tag], axis=1) & new_df['IS_ERROR'] == True, 'ERROR_REASON'] = tag\n",
    "    print(tag)\n",
    "    print(reasons[reasons['TAG']==tag]['DESCRIPTION'].values[0])\n",
    "    # print(new_df[new_df['ERROR_REASON']==tag].info())\n",
    "    print(new_df[new_df['ERROR_REASON']==tag].shape[0])\n",
    "\n",
    "check_df = copy(new_df[new_df['IS_ERROR'] == True & new_df['ERROR_REASON'].isna()])\n",
    "print(\"Result MisMatch: \", check_df[check_df['ERROR_MSG'] == 'Result MisMatch'].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = new_df[new_df['IS_ERROR'] == True]\n",
    "sample_100 = errors.sample(n=100, random_state=233)\n",
    "output_path = f\"output/{dbms_name}_duckdb_sample_100.csv\"\n",
    "output_reason_path = f\"output/{dbms_name}_duckdb_sample_100_reason.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_100.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = sample_100.iloc[cnt]\n",
    "\n",
    "print(\"-----------I have the schema generated as follows:---------\")\n",
    "temp_df = new_df[new_df['TESTFILE_PATH'] == sample['TESTFILE_PATH']]\n",
    "for sql in temp_df[temp_df['CASE_TYPE'] == 'Statement']['SQL']:\n",
    "    print(sql + ';')\n",
    "print(\"-----------The SQL commands is:---------\")\n",
    "print(sample.SQL)\n",
    "print(\"-----------The expected result is:---------\")\n",
    "print(sample.EXPECTED_RESULT)\n",
    "print(\"-----------The actual result is:---------\")\n",
    "print(sample.ACTUAL_RESULT)\n",
    "print(\"-----------The error message is:---------\")\n",
    "print(sample.ERROR_MSG)\n",
    "print(sample.LOGS_INDEX)\n",
    "cnt += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample['TESTFILE_PATH'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_100_reason = pd.read_csv(output_path)\n",
    "visible_reasons = sample_100_reason[[\n",
    "    'SQL', 'ERROR_REASON', 'ERROR_MSG', 'ACTUAL_RESULT', 'EXPECTED_RESULT',]]\n",
    "reasons_mapping = pd.read_csv('data/duckdb_suite_errors.csv')\n",
    "reasons_mapping = reasons_mapping[reasons_mapping['DBMS'] == dbms_name]\n",
    "# add columns to sample_100_reason according to reasons_mapping\n",
    "visible_reasons = pd.merge(visible_reasons, reasons_mapping,\n",
    "                           right_on='TAG', left_on='ERROR_REASON', how='inner')\n",
    "# visible_reasons.drop(columns=['TAG'], inplace=True)\n",
    "visible_reasons.to_csv(output_reason_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reason_summary = visible_reasons[['SUPER_TAG',  'DESCRIPTION']].groupby(['SUPER_TAG']).count().style.to_latex()\n",
    "print(reason_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbms_name = 'mysql'\n",
    "\n",
    "output_path = f\"output/{dbms_name}_duckdb_sample_100.csv\"\n",
    "output_reason_path = f\"output/{dbms_name}_duckdb_sample_100_reason.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.346919157935211\n",
      "Queries: 7818, Statements: 5770\n",
      "Wrong Queries: 358\n"
     ]
    }
   ],
   "source": [
    "# open a csv file and read the content\n",
    "df = pd.read_csv(f\"output/{dbms_name}_duckdb_filter_results.csv\")\n",
    "result_log = pd.read_csv(f\"output/{dbms_name}_duckdb_filter_logs.csv\")\n",
    "error_percentage = df[df['IS_ERROR'] == True].shape[0] / df.shape[0]\n",
    "print(1 - error_percentage)\n",
    "error_queries = df[(df['CASE_TYPE'] == 'Query') &\n",
    "                   (df['IS_ERROR'] == True)].shape[0]\n",
    "error_statements = df[(df['CASE_TYPE'] == 'Statement') &\n",
    "                      (df['IS_ERROR'] == True)].shape[0]\n",
    "wrong_queries = df[(df['CASE_TYPE'] == 'Query') & (\n",
    "    df['ERROR_MSG'] == 'Result MisMatch')].shape[0]\n",
    "print(f\"Queries: {error_queries}, Statements: {error_statements}\")\n",
    "print(f\"Wrong Queries: {wrong_queries}\")\n",
    "\n",
    "# add empty column ERROR_REASON to df\n",
    "df['ERROR_REASON'] = None\n",
    "check_df = df.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pick_a_sample(check_df, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import utils\n",
    "from copy import copy\n",
    "import re\n",
    "\n",
    "TEMP_FILTER = {\n",
    "    'TYPE_CAST': lambda x: re.match(r'1064 \\(42000\\).*\\'::', str(x['ERROR_MSG'])) is not None,\n",
    "    'GENERAL': lambda x: re.match(r'1305 \\(42000\\)', str(x['ERROR_MSG'])) is not None,\n",
    "}\n",
    "\n",
    "new_df = copy(df)\n",
    "reasons = pd.read_csv(\"data/duckdb_suite_errors.csv\")\n",
    "tags = reasons[reasons['DBMS'] == dbms_name]['TAG'].values.tolist()\n",
    "for tag in tags:\n",
    "    if tag not in TEMP_FILTER:\n",
    "        print(f\"Tag {tag} is not in the test filter, implement it first!\")\n",
    "        continue\n",
    "    new_df.loc[new_df.apply(TEMP_FILTER[tag], axis=1) & new_df['IS_ERROR'] == True, 'ERROR_REASON'] = tag\n",
    "    print(tag)\n",
    "    print(reasons[reasons['TAG']==tag]['DESCRIPTION'].values[0])\n",
    "    # print(new_df[new_df['ERROR_REASON']==tag].info())\n",
    "    print(new_df[new_df['ERROR_REASON']==tag].shape[0])\n",
    "\n",
    "check_df = copy(new_df[new_df['IS_ERROR'] == True & new_df['ERROR_REASON'].isna()])\n",
    "print(\"Result MisMatch: \", check_df[check_df['ERROR_MSG'] == 'Result MisMatch'].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = new_df[new_df['IS_ERROR'] == True]\n",
    "sample_100 = errors.sample(n=100, random_state=233)\n",
    "output_path = f\"output/{dbms_name}_duckdb_sample_100.csv\"\n",
    "output_reason_path = f\"output/{dbms_name}_duckdb_sample_100_reason.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_100.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = sample_100.iloc[cnt]\n",
    "\n",
    "print(\"-----------I have the schema generated as follows:---------\")\n",
    "temp_df = new_df[new_df['TESTFILE_PATH'] == sample['TESTFILE_PATH']]\n",
    "for sql in temp_df[temp_df['CASE_TYPE'] == 'Statement']['SQL']:\n",
    "    print(sql + ';')\n",
    "print(\"-----------The SQL commands is:---------\")\n",
    "print(sample.SQL)\n",
    "print(\"-----------The expected result is:---------\")\n",
    "print(sample.EXPECTED_RESULT)\n",
    "print(\"-----------The actual result is:---------\")\n",
    "print(sample.ACTUAL_RESULT)\n",
    "print(\"-----------The error message is:---------\")\n",
    "print(sample.ERROR_MSG)\n",
    "print(sample.LOGS_INDEX)\n",
    "cnt += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample['TESTFILE_PATH'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_100_reason = pd.read_csv(output_path)\n",
    "visible_reasons = sample_100_reason[[\n",
    "    'SQL', 'ERROR_REASON', 'ERROR_MSG', 'ACTUAL_RESULT', 'EXPECTED_RESULT',]]\n",
    "reasons_mapping = pd.read_csv('data/duckdb_suite_errors.csv')\n",
    "reasons_mapping = reasons_mapping[reasons_mapping['DBMS'] == dbms_name]\n",
    "# add columns to sample_100_reason according to reasons_mapping\n",
    "visible_reasons = pd.merge(visible_reasons, reasons_mapping,\n",
    "                           right_on='TAG', left_on='ERROR_REASON', how='inner')\n",
    "# visible_reasons.drop(columns=['TAG'], inplace=True)\n",
    "visible_reasons.to_csv(output_reason_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reason_summary = visible_reasons[['SUPER_TAG',  'DESCRIPTION']].groupby(['SUPER_TAG']).count().style.to_latex()\n",
    "print(reason_summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
