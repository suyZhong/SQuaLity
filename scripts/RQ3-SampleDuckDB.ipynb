{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.chdir('../')\n",
    "SCRIPDIR = os.path.dirname(os.path.abspath(\"test.ipynb\"))\n",
    "sys.path.append(os.path.dirname(SCRIPDIR))\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from src.utils import SETUP_PATH, convert_testfile_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_results = pd.read_csv('output/duckdb_duckdb_results.csv')\n",
    "results = original_results.copy() \n",
    "# change the TESTFILE_PATH in results\n",
    "results['TESTFILE_NAME'] = results['TESTFILE_PATH'].apply(lambda x: convert_testfile_name(x, 'duckdb'))    \n",
    "\n",
    "path = SETUP_PATH['filter']\n",
    "filter_df = pd.concat([pd.read_csv(f\"{path}/{file}\") for file in os.listdir(\n",
    "    path) if file.endswith('.csv')], ignore_index=True)\n",
    "filter_df[['TESTCASE_INDEX', 'CLUSTER']] = filter_df[[\n",
    "    'TESTCASE_INDEX', 'CLUSTER']].astype(int)\n",
    "\n",
    "# results_filtered = results[results['TESTFILE_NAME'].isin(filter_df['TESTFILE_NAME'])]\n",
    "results_filtered = results.merge(filter_df, on='TESTFILE_NAME', how='inner')\n",
    "# results_filtered = results_filtered[results_filtered\n",
    "filtered = results_filtered[(results_filtered['TESTCASE_INDEX_x'] == results_filtered['TESTCASE_INDEX_y']) | (results_filtered['TESTCASE_INDEX_y'] == -1)].drop(columns=['TESTCASE_INDEX_y', 'CLUSTER', 'TESTFILE_NAME']).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_30 = errors.sample(n=30)\n",
    "errors = original_results[original_results['IS_ERROR'] == True]\n",
    "sample_100 = errors.sample(n=100, random_state=233)\n",
    "sample_100['ERROR_REASON'] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre labeling\n",
    "import re\n",
    "sample_100.loc[sample_100.apply(lambda x: x['ERROR_MSG'].find('__TEST_DIR__') != -1, axis=1), 'ERROR_REASON'] = 1\n",
    "sample_100.loc[sample_100.apply(lambda x: str(x['ACTUAL_RESULT']).find(\"{'key'}\") != -1, axis=1), 'ERROR_REASON'] = 2\n",
    "sample_100.loc[sample_100.apply(lambda x: re.search(r\"\\{.*\\}\", str(x['ACTUAL_RESULT']), re.DOTALL) is not None and re.search(r\"\\{.*\\}\", str(x['EXPECTED_RESULT']), re.DOTALL) is not None, axis=1), 'ERROR_REASON'] = 2\n",
    "sample_100.loc[sample_100.apply(lambda x: re.search(r\"\\[.*\\]\", str(x['ACTUAL_RESULT']), re.DOTALL) is not None and re.search(r\"\\[].*\\]\", str(x['EXPECTED_RESULT']), re.DOTALL) is not None, axis=1), 'ERROR_REASON'] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"output/duckdb_sample_100.csv\"\n",
    "output_reason_path = \"output/duckdb_sample_100_reason.csv\"\n",
    "sample_100.to_csv(output_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------I have the schema generated as follows:---------\n",
      "PRAGMA enable_verification;\n",
      "SELECT strptime('', '');\n",
      "SELECT strptime(NULL, '');\n",
      "SELECT strptime('10.28.1910', ['%d-%m-%Y', '%m-%d-%Y', '%d/%m/%Y', '%m/%d/%Y'])  Error: Literal does not match, expected /;\n",
      "SELECT strptime('Mon Oct 17 2022 22:00:00 GMT+0000 (GMT)', '%a %b %d %Y %X GMT%z (%Z') as broken;\n",
      "select strptime('2020-12-31 21:25:58.745232+0', '%Y-%m-%d %H:%M:%S.%f%z');\n",
      "select strptime('2020-12-31 21:25:58.745232+0X', '%Y-%m-%d %H:%M:%S.%f%z');\n",
      "select strptime('2020-12-31 21:25:58.745232+X0', '%Y-%m-%d %H:%M:%S.%f%z');\n",
      "select strptime('2020-12-31 21:25:58.745232+000', '%Y-%m-%d %H:%M:%S.%f%z');\n",
      "select strptime('2020-12-31 21:25:58.745232X00', '%Y-%m-%d %H:%M:%S.%f%z');\n",
      "SELECT strptime('2018-20-10', '%Y-%m-%d');\n",
      "SELECT strptime('2018-10-100', '%Y-%m-%d');\n",
      "SELECT strptime('969-10-10', '%y-%m-%d');\n",
      "SELECT strptime('2000/10/10', '%Y-%m-%d');\n",
      "SELECT strptime('2001-02-30', '%Y-%m-%d');\n",
      "SELECT strptime('2000-10-hello', '%Y-%m-%d');\n",
      "SELECT strptime('2000-10-01 24:00:00', '%Y-%m-%d %H:%M:%S');\n",
      "SELECT strptime('2000-10-01 00:00:00 AM', '%Y-%m-%d %I:%M:%S %p');\n",
      "SELECT strptime('2000-10-01 13:00:00 AM', '%Y-%m-%d %I:%M:%S %p');\n",
      "SELECT strptime('2000-10-01 23:60:00', '%Y-%m-%d %H:%M:%S');\n",
      "SELECT strptime('2000-10-01 23:59:60', '%Y-%m-%d %H:%M:%S');\n",
      "SELECT strptime('2000-10-01 23:59:59.10000000', '%Y-%m-%d %H:%M:%S.%f');\n",
      "SELECT strptime('2000-10-01 23:59:59.1000000000000000000000000000', '%Y-%m-%d %H:%M:%S.%f');\n",
      "SELECT strptime('', '%Y-%m-%d %H:%M:%S.%f');\n",
      "SELECT strptime('', '%p');\n",
      "SELECT strptime('a', '%p');\n",
      "SELECT strptime('mp', '%p');\n",
      "SELECT strptime('pp', '%p');\n",
      "SELECT strptime('zm', '%p');\n",
      "SELECT strptime('moa', '%a');\n",
      "SELECT strptime('moaday', '%A');\n",
      "SELECT strptime('mondayy', '%A');\n",
      "SELECT strptime('juk', '%b');\n",
      "SELECT strptime('juke', '%B');\n",
      "SELECT strptime('junee', '%B');\n",
      "SELECT strptime('500', '%j');\n",
      "SELECT strptime('500', '%-j');\n",
      "SELECT strptime('0', '%j');\n",
      "SELECT strptime('0', '%-j');\n",
      "SELECT strptime('60', '%U');\n",
      "SELECT strptime('60', '%W');\n",
      "SELECT strptime('9', '%w');\n",
      "SELECT strptime('20 19', '%U %W');\n",
      "SELECT strptime('20 19', '%U %j');\n",
      "SELECT strptime('20 19', '%W %j');\n",
      "SELECT strptime('Mon 30, December 30, 20:3:5 PM', '%a %d, %B %y, %H:%M:%S %p');\n",
      "SELECT strptime('21/10/2018', '%-q/%m/%Y');\n",
      "SELECT strptime('9999999', '%f');\n",
      "SELECT strptime('9999', '%g');\n",
      "SELECT strptime('2000/10/10', random()::varchar);\n",
      "-----------The SQL commands is:---------\n",
      "SELECT strptime('Mon Oct 17 2022 22:00:00 GMT+0000 (GMT)', '%a %b %d %Y %X GMT%z (%Z)') as fixed;\n",
      "-----------The expected result is:---------\n",
      "2022-10-17 22:00:00+00\n",
      "-----------The actual result is:---------\n",
      "2022-10-17 22:00:00\n",
      "-----------The error message is:---------\n",
      "Result MisMatch\n",
      "142\n"
     ]
    }
   ],
   "source": [
    "sample = sample_100.iloc[cnt]\n",
    "\n",
    "print(\"-----------I have the schema generated as follows:---------\")\n",
    "temp_df = results[results['TESTFILE_PATH'] == sample['TESTFILE_PATH']]\n",
    "for sql in temp_df[temp_df['CASE_TYPE'] == 'Statement']['SQL']:\n",
    "    print(sql + ';')\n",
    "print(\"-----------The SQL commands is:---------\")\n",
    "print(sample.SQL)\n",
    "print(\"-----------The expected result is:---------\")\n",
    "print(sample.EXPECTED_RESULT)\n",
    "print(\"-----------The actual result is:---------\")\n",
    "print(sample.ACTUAL_RESULT)\n",
    "print(\"-----------The error message is:---------\")\n",
    "print(sample.ERROR_MSG)\n",
    "print(sample.LOGS_INDEX)\n",
    "cnt += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duckdb_tests/sql/upsert/postgres/single_key.test\n",
      "Not found\n"
     ]
    }
   ],
   "source": [
    "# print the sample\n",
    "print(sample.TESTFILE_PATH)\n",
    "\n",
    "# open the file and see if there's a pattern inside\n",
    "with open(sample.TESTFILE_PATH) as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        if \"con1\" in line:\n",
    "            print(line)\n",
    "            break\n",
    "    else:\n",
    "        print(\"Not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_100_reason = pd.read_csv(output_path)\n",
    "visible_reasons = sample_100_reason[['SQL','ERROR_REASON', 'ERROR_MSG', 'ACTUAL_RESULT','EXPECTED_RESULT',  ]]\n",
    "reasons_mapping = pd.read_csv('data/error_reason_manual.csv')\n",
    "\n",
    "# add columns to sample_100_reason according to reasons_mapping\n",
    "visible_reasons = pd.merge(visible_reasons, reasons_mapping, right_on='index',left_on = 'ERROR_REASON', how='inner')\n",
    "visible_reasons.drop(columns=['index'], inplace=True)\n",
    "visible_reasons.to_csv(output_reason_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{llr}\n",
      " &  & DESCRIPTION \\\\\n",
      "SUPER_TAG & TAG &  \\\\\n",
      "\\multirow[c]{2}{*}{Client} & Data Type & 66 \\\\\n",
      " & Numeric & 9 \\\\\n",
      "Environment & File Paths & 20 \\\\\n",
      "\\multirow[c]{2}{*}{Misc} & Function & 1 \\\\\n",
      " & Parser & 4 \\\\\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reason_summary = visible_reasons[['SUPER_TAG', 'TAG', 'DESCRIPTION']].groupby(['SUPER_TAG', 'TAG']).count().style.to_latex()\n",
    "print(reason_summary)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
