{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.chdir('../')\n",
    "SCRIPDIR = os.path.dirname(os.path.abspath(\"test.ipynb\"))\n",
    "sys.path.append(os.path.dirname(SCRIPDIR))\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_a_sample(check_df: pd.DataFrame, result_log: pd.DataFrame = None):\n",
    "    if check_df[check_df['ERROR_REASON'].isna()].shape[0] == 0:\n",
    "        print(\"No error found!\")\n",
    "        return None\n",
    "    sample = check_df[check_df['ERROR_REASON'].isna()].sample()\n",
    "    if result_log is not None:\n",
    "        print(\"-----------I have the schema generated as follows:---------\")\n",
    "        logs = result_log.iloc[sample.LOGS_INDEX.values[0]]\n",
    "        # temp_df = df[df['TESTFILE_PATH'] == sample['TESTFILE_PATH'].values[0]]\n",
    "        # for sql in temp_df[temp_df['CASE_TYPE'] == 'Statement']['SQL']:\n",
    "        #     print(sql)\n",
    "        print(logs.values[0])\n",
    "    print(\"-----------The SQL commands is:---------\")\n",
    "    print(sample.SQL.values[0])\n",
    "    print(\"-----------The expected result is:---------\")\n",
    "    print(sample.EXPECTED_RESULT.values[0])\n",
    "    print(\"-----------The actual result is:---------\")\n",
    "    print(sample.ACTUAL_RESULT.values[0])\n",
    "    print(\"-----------The error message is:---------\")\n",
    "    print(sample.ERROR_MSG.values[0])\n",
    "\n",
    "    print(\"-----------The test file is:---------\")\n",
    "    print(sample.TESTFILE_PATH.values[0])\n",
    "    print(\"___________The index is:___________\")\n",
    "    print(sample.index.values[0])\n",
    "    return sample.index.values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DuckDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbms_name = 'duckdb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open a csv file and read the content\n",
    "df = pd.read_csv(f\"output/{dbms_name}_sqlite_results.csv\")\n",
    "result_log = pd.read_csv(f\"output/{dbms_name}_sqlite_logs.csv\")\n",
    "\n",
    "# add empty column ERROR_REASON to df\n",
    "df['ERROR_REASON'] = None\n",
    "check_df = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pick_a_sample(check_df, result_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import utils\n",
    "from copy import copy\n",
    "import re\n",
    "\n",
    "TEMP_FILTER = {\n",
    "    'INT_DIV' : lambda x: re.search('/', str(x['SQL'])) is not None and x['ERROR_MSG'] == 'Result MisMatch', \n",
    "    'VARCHAR_SYNTAX': lambda x: str.startswith(str(x['ERROR_MSG']), \"Binder Error: No function matches the given name and argument types '+(VARCHAR)'\"),\n",
    "    'COL_IN_AGG': lambda x: re.match(r\"Binder Error: column .* must appear in the GROUP BY clause\", x['ERROR_MSG']) is not None,\n",
    "    'TRIGGER': lambda x: re.match(r\"CREATE TRIGGER|DROP TRIGGER\", x['SQL'], re.IGNORECASE) is not None,\n",
    "    # 'UPDATE_MUL': lambda x: re.match(r\"Binder Error: Multiple assignments to same column\", x['ERROR_MSG']) is not None,\n",
    "    'INTEGER': lambda x: re.match(\n",
    "        r\"Out of Range Error: Overflow in multiplication of INT32 \", x['ERROR_MSG']) is not None,\n",
    "    'REPLACE': lambda x: x['TESTFILE_PATH'] == 'sqlite_tests/evidence/slt_lang_replace.test',\n",
    "    'UPDATE_MUL': lambda x: x['TESTFILE_PATH'] == 'sqlite_tests/evidence/slt_lang_update.test',\n",
    "    'REINDEX': lambda x: x['TESTFILE_PATH'] == 'sqlite_tests/evidence/slt_lang_reindex.test',\n",
    "    'EMPTY_SET': lambda x: re.match(r'Parser Error: .* \"\\)\"', x['ERROR_MSG']) is not None,\n",
    "}\n",
    "\n",
    "new_df = copy(df)\n",
    "reasons = pd.read_csv(\"data/sqlite_suite_errors.csv\")\n",
    "tags = reasons[reasons['DBMS'] == dbms_name]['TAG'].values.tolist()\n",
    "for tag in tags:\n",
    "    if tag not in TEMP_FILTER:\n",
    "        print(f\"Tag {tag} is not in the test filter, implement it first!\")\n",
    "        continue\n",
    "    new_df.loc[new_df.apply(TEMP_FILTER[tag], axis=1) & new_df['IS_ERROR'] == True, 'ERROR_REASON'] = tag\n",
    "    print(tag)\n",
    "    print(reasons[reasons['TAG']==tag]['REASON'].values[0])\n",
    "    # print(new_df[new_df['ERROR_REASON']==tag].info())\n",
    "    print(new_df[new_df['ERROR_REASON']==tag].shape[0])\n",
    "\n",
    "check_df = copy(new_df[new_df['IS_ERROR'] == True & new_df['ERROR_REASON'].isna()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_index = pick_a_sample(check_df, result_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually reason the remaining errors\n",
    "manual_reason = \"REPLACE\"\n",
    "\n",
    "new_df.loc[sample_index, 'ERROR_REASON'] = manual_reason\n",
    "check_df.loc[sample_index, 'ERROR_REASON'] = manual_reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.to_csv(f\"output/{dbms_name}_sqlite_results_after.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of errors for each reason in new_df\n",
    "new_df = pd.read_csv(f\"output/{dbms_name}_sqlite_results_after.csv\")\n",
    "new_df.loc[new_df['IS_ERROR'] == False, 'ERROR_REASON'] = 'SUCCESS'\n",
    "reason_df = new_df[['SQL', 'EXPECTED_RESULT', 'ACTUAL_RESULT','ERROR_REASON', 'IS_ERROR']]\n",
    "reason_summary = reason_df['ERROR_REASON'].value_counts()\n",
    "reason_summary.plot(kind='bar')\n",
    "plt.xlabel(\"Error Reasons\")\n",
    "plt.ylabel(\"Number of Errors\")\n",
    "plt.title(\"SQLite - DuckDB Test Suite Error Reasons Distribution\")\n",
    "# plt.axis(rotation=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbms_name = 'postgresql'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open a csv file and read the content\n",
    "df = pd.read_csv(f\"output/{dbms_name}_sqlite_results.csv\")\n",
    "result_log = pd.read_csv(f\"output/{dbms_name}_sqlite_logs.csv\")\n",
    "\n",
    "# add empty column ERROR_REASON to df\n",
    "df['ERROR_REASON'] = None\n",
    "check_df = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pick_a_sample(check_df, result_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import utils\n",
    "from copy import copy\n",
    "import re\n",
    "\n",
    "TEMP_FILTER = {\n",
    "    'VARCHAR_SYNTAX': lambda x: re.match(r\"operator does not exist: \\+ text\", x['ERROR_MSG']) is not None,\n",
    "    'VIEW_DEPENDENCY': lambda x: re.match(r\"cannot drop view .* because other objects depend on it\", x['ERROR_MSG']) is not None,\n",
    "    'DIV_ZERO': lambda x: re.match(r\"division by zero\", x['ERROR_MSG']) is not None,\n",
    "    'TRIGGER': lambda x: re.match(r\"CREATE TRIGGER|DROP TRIGGER\", x['SQL']) is not None and re.match(r\"syntax error\", x['ERROR_MSG']) is not None,\n",
    "    'NULLIF': lambda x: re.search(r\"NULLIF\", x['SQL']) is not None,\n",
    "    'CAST': lambda x: re.search(r\"CAST\", x['SQL']) is not None,\n",
    "    'COALESCE': lambda x: re.search(r\"COALESCE\", x['SQL']) is not None,\n",
    "    'INTEGER': lambda x: re.search(r\"integer out of range\", x['ERROR_MSG']) is not None,\n",
    "    'EMPTY_SET': lambda x: re.match(r'syntax error at or near \"\\)\"', x['ERROR_MSG']) is not None,\n",
    "    'REPLACE': lambda x: x['TESTFILE_PATH'] == 'sqlite_tests/evidence/slt_lang_replace.test',\n",
    "    'UPDATE_MUL': lambda x: x['TESTFILE_PATH'] == 'sqlite_tests/evidence/slt_lang_update.test',\n",
    "    'REINDEX': lambda x: x['TESTFILE_PATH'] == 'sqlite_tests/evidence/slt_lang_reindex.test',\n",
    "    'TYPE_CAST': lambda x: re.search(r\"SELECT.*'.*'.*IN\\s*\\(.+\\)\", x['SQL']) is not None,\n",
    "    'CASE_END': lambda x: re.search(r\"CASE.*END\", x['SQL']) is not None,\n",
    "    'OPERATE_VIEW': lambda x: x['TESTFILE_PATH'] == 'sqlite_tests/evidence/slt_lang_createview.test' and x['ACTUAL_RESULT'] == 'True',\n",
    "}\n",
    "\n",
    "new_df = copy(df)\n",
    "reasons = pd.read_csv(\"data/sqlite_suite_errors.csv\")\n",
    "tags = reasons[reasons['DBMS'] == dbms_name]['TAG'].values.tolist()\n",
    "for tag in tags:\n",
    "    if tag not in TEMP_FILTER:\n",
    "        print(f\"Tag {tag} is not in the test filter, implement it first!\")\n",
    "        continue\n",
    "    new_df.loc[new_df.apply(TEMP_FILTER[tag], axis=1) & new_df['IS_ERROR'] == True, 'ERROR_REASON'] = tag\n",
    "    print(tag)\n",
    "    print(reasons[reasons['TAG']==tag]['REASON'].values[0])\n",
    "    # print(new_df[new_df['ERROR_REASON']==tag].info())\n",
    "    print(new_df[new_df['ERROR_REASON']==tag].shape[0])\n",
    "\n",
    "check_df = copy(new_df[new_df['IS_ERROR'] == True & new_df['ERROR_REASON'].isna()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_index = pick_a_sample(check_df, result_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually reason the remaining errors\n",
    "manual_reason = \"REPLACE\"\n",
    "\n",
    "new_df.loc[sample_index, 'ERROR_REASON'] = manual_reason\n",
    "check_df.loc[sample_index, 'ERROR_REASON'] = manual_reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.to_csv(f\"output/{dbms_name}_sqlite_results_after.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of errors for each reason in new_df\n",
    "new_df = pd.read_csv(f\"output/{dbms_name}_sqlite_results_after.csv\")\n",
    "new_df.loc[new_df['IS_ERROR'] == False, 'ERROR_REASON'] = 'SUCCESS'\n",
    "reason_df = new_df[['SQL', 'EXPECTED_RESULT', 'ACTUAL_RESULT','ERROR_REASON', 'IS_ERROR']]\n",
    "reason_summary = reason_df['ERROR_REASON'].value_counts()\n",
    "reason_summary.plot(kind='bar')\n",
    "plt.xlabel(\"Error Reasons\")\n",
    "plt.ylabel(\"Number of Errors\")\n",
    "plt.title(\"SQLite - PostgreSQL Test Suite Error Reasons Distribution\")\n",
    "# plt.axis(rotation=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbms_name = 'mysql'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open a csv file and read the content\n",
    "df = pd.read_csv(f\"output/{dbms_name}_sqlite_results.csv\")\n",
    "result_log = pd.read_csv(f\"output/{dbms_name}_sqlite_logs.csv\")\n",
    "\n",
    "# add empty column ERROR_REASON to df\n",
    "df['ERROR_REASON'] = None\n",
    "check_df = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pick_a_sample(check_df, result_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import utils\n",
    "from copy import copy\n",
    "import re\n",
    "\n",
    "TEMP_FILTER = {\n",
    "    'COL_IN_AGG': lambda x: re.match(r\"1055 \\(42000\\)\", x['ERROR_MSG']) is not None,\n",
    "    # 'VARCHAR_SYNTAX': lambda x: re.match(r\"operator does not exist: \\+ text\", x['ERROR_MSG']) is not None,\n",
    "    # 'VIEW_DEPENDENCY': lambda x: re.match(r\"cannot drop view .* because other objects depend on it\", x['ERROR_MSG']) is not None,\n",
    "    # 'DIV_ZERO': lambda x: re.match(r\"division by zero\", x['ERROR_MSG']) is not None,\n",
    "    'TRIGGER': lambda x: re.match(r\"CREATE TRIGGER|DROP TRIGGER\", x['SQL']) is not None,\n",
    "    # 'NULLIF': lambda x: re.search(r\"NULLIF\", x['SQL']) is not None,\n",
    "    # 'CAST': lambda x: re.search(r\"CAST\", x['SQL']) is not None,\n",
    "    # 'COALESCE': lambda x: re.search(r\"COALESCE\", x['SQL']) is not None,\n",
    "    # 'INTEGER': lambda x: re.search(r\"integer out of range\", x['ERROR_MSG']) is not None,\n",
    "    # 'EMPTY_SET': lambda x: re.match(r'syntax error at or near \"\\)\"', x['ERROR_MSG']) is not None,\n",
    "    # 'REPLACE': lambda x: x['TESTFILE_PATH'] == 'sqlite_tests/evidence/slt_lang_replace.test',\n",
    "    # 'UPDATE_MUL': lambda x: x['TESTFILE_PATH'] == 'sqlite_tests/evidence/slt_lang_update.test',\n",
    "    # 'REINDEX': lambda x: x['TESTFILE_PATH'] == 'sqlite_tests/evidence/slt_lang_reindex.test',\n",
    "    # 'TYPE_CAST': lambda x: re.search(r\"SELECT.*'.*'.*IN\\s*\\(.+\\)\", x['SQL']) is not None,\n",
    "    # 'CASE_END': lambda x: re.search(r\"CASE.*END\", x['SQL']) is not None,\n",
    "    'OPERATE_VIEW': lambda x: x['TESTFILE_PATH'] == 'sqlite_tests/evidence/slt_lang_createview.test' and x['ACTUAL_RESULT'] == True,\n",
    "    'TIMEOUT': lambda x: re.match(r\"Time Exceed\", x['ERROR_MSG']) is not None,\n",
    "    'DROP_INDEX': lambda x: re.match(r\"DROP INDEX\", x['SQL'], re.IGNORECASE) is not None,\n",
    "}\n",
    "\n",
    "new_df = copy(df)\n",
    "reasons = pd.read_csv(\"data/sqlite_suite_errors.csv\")\n",
    "tags = reasons[reasons['DBMS'] == dbms_name]['TAG'].values.tolist()\n",
    "for tag in tags:\n",
    "    if tag not in TEMP_FILTER:\n",
    "        print(f\"Tag {tag} is not in the test filter, implement it first!\")\n",
    "        continue\n",
    "    new_df.loc[new_df.apply(TEMP_FILTER[tag], axis=1) & new_df['IS_ERROR'] == True, 'ERROR_REASON'] = tag\n",
    "    print(tag)\n",
    "    print(reasons[reasons['TAG']==tag]['REASON'].values[0])\n",
    "    # print(new_df[new_df['ERROR_REASON']==tag].info())\n",
    "    print(new_df[new_df['ERROR_REASON']==tag].shape[0])\n",
    "\n",
    "check_df = copy(new_df[new_df['IS_ERROR'] == True & new_df['ERROR_REASON'].isna()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_index = pick_a_sample(check_df, result_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually reason the remaining errors\n",
    "manual_reason = \"REPLACE\"\n",
    "\n",
    "new_df.loc[sample_index, 'ERROR_REASON'] = manual_reason\n",
    "check_df.loc[sample_index, 'ERROR_REASON'] = manual_reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.to_csv(f\"output/{dbms_name}_sqlite_results_after.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of errors for each reason in new_df\n",
    "new_df = pd.read_csv(f\"output/{dbms_name}_sqlite_results_after.csv\")\n",
    "new_df.loc[new_df['IS_ERROR'] == False, 'ERROR_REASON'] = 'SUCCESS'\n",
    "reason_df = new_df[['SQL', 'EXPECTED_RESULT', 'ACTUAL_RESULT','ERROR_REASON', 'IS_ERROR']]\n",
    "reason_summary = reason_df['ERROR_REASON'].value_counts()\n",
    "reason_summary.plot(kind='bar')\n",
    "plt.xlabel(\"Error Reasons\")\n",
    "plt.ylabel(\"Number of Errors\")\n",
    "plt.title(\"SQLite - PostgreSQL Test Suite Error Reasons Distribution\")\n",
    "# plt.axis(rotation=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reasons = pd.read_csv(\"data/sqlite_suite_errors.csv\")\n",
    "\n",
    "duckdb_results = pd.read_csv(\"output/duckdb_sqlite_results_after.csv\")\n",
    "reason_df = duckdb_results[['SQL', 'EXPECTED_RESULT',\n",
    "                    'ACTUAL_RESULT', 'ERROR_REASON', 'IS_ERROR']]\n",
    "duckdb_reasons = reason_df['ERROR_REASON'].value_counts()\n",
    "\n",
    "\n",
    "postgresql_results = pd.read_csv(\"output/postgresql_sqlite_results_after.csv\")\n",
    "reason_df = postgresql_results[['SQL', 'EXPECTED_RESULT',\n",
    "                    'ACTUAL_RESULT', 'ERROR_REASON', 'IS_ERROR']]\n",
    "postgresql_reasons = reason_df['ERROR_REASON'].value_counts()\n",
    "\n",
    "# mysql\n",
    "mysql_results = pd.read_csv(\"output/mysql_sqlite_results_after.csv\")\n",
    "reason_df = mysql_results[['SQL', 'EXPECTED_RESULT',\n",
    "                    'ACTUAL_RESULT', 'ERROR_REASON', 'IS_ERROR']]\n",
    "mysql_reasons = reason_df['ERROR_REASON'].value_counts()\n",
    "\n",
    "print(duckdb_reasons)\n",
    "print(postgresql_reasons)\n",
    "# outer join two dataframes\n",
    "\n",
    "join_df = pd.concat([duckdb_reasons, postgresql_reasons, mysql_reasons], axis=1, join='outer')\n",
    "# print(join_df)\n",
    "\n",
    "join_df.fillna(0, inplace=True)\n",
    "join_df = join_df.astype(int)\n",
    "\n",
    "# pure_reasons = reasons[['TAG', 'REASON']].drop_duplicates()\n",
    "pure_reasons = reasons[['TAG', 'SUPER_TAG', 'REASON']].drop_duplicates()\n",
    "# join the join_df and the reasons dataframe, drop duplicate rows\n",
    "join_df = join_df.join(pure_reasons.set_index('TAG'),  how='left')\n",
    "join_df.sort_values(by=['SUPER_TAG'], inplace=True)\n",
    "# sum the rows with the same SUPER_TAG\n",
    "join_df.reset_index(inplace=True, drop=True)\n",
    "join_df = join_df.groupby('SUPER_TAG').sum()\n",
    "join_df.to_latex(\"output/error_reasons_highlevel.tex\", )\n",
    "# join_df.style.to_latex(\"output/error_reasons.tex\", )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INT_DIV           104033\n",
      "VARCHAR_SYNTAX      7075\n",
      "COL_IN_AGG          1278\n",
      "INTEGER               43\n",
      "TRIGGER               23\n",
      "EMPTY_SET              8\n",
      "REPLACE                4\n",
      "UPDATE_MUL             3\n",
      "REINDEX                1\n",
      "Name: ERROR_REASON, dtype: int64\n",
      "VARCHAR_SYNTAX     6065\n",
      "VIEW_DEPENDENCY    4860\n",
      "CAST                387\n",
      "CASE_END            114\n",
      "COALESCE             82\n",
      "NULLIF               26\n",
      "TRIGGER              23\n",
      "INTEGER              12\n",
      "REPLACE               8\n",
      "EMPTY_SET             8\n",
      "TYPE_CAST             4\n",
      "UPDATE_MUL            3\n",
      "OPERATE_VIEW          2\n",
      "REINDEX               1\n",
      "Name: ERROR_REASON, dtype: int64\n",
      "Command      9\n",
      "Function     4\n",
      "Operator     3\n",
      "IOperator    1\n",
      "Type         1\n",
      "Issue        1\n",
      "Name: SUPER_TAG, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1418548/1453108505.py:45: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  join_df.to_latex(\"output/error_reasons_highlevel.tex\", )\n"
     ]
    }
   ],
   "source": [
    "reasons = pd.read_csv(\"data/sqlite_suite_errors.csv\")\n",
    "\n",
    "duckdb_results = pd.read_csv(\"output/duckdb_sqlite_results_after.csv\")\n",
    "reason_df = duckdb_results[['SQL', 'EXPECTED_RESULT',\n",
    "                    'ACTUAL_RESULT', 'ERROR_REASON', 'IS_ERROR']]\n",
    "duckdb_reasons = reason_df['ERROR_REASON'].value_counts()\n",
    "\n",
    "\n",
    "postgresql_results = pd.read_csv(\"output/postgresql_sqlite_results_after.csv\")\n",
    "reason_df = postgresql_results[['SQL', 'EXPECTED_RESULT',\n",
    "                    'ACTUAL_RESULT', 'ERROR_REASON', 'IS_ERROR']]\n",
    "postgresql_reasons = reason_df['ERROR_REASON'].value_counts()\n",
    "\n",
    "# mysql\n",
    "mysql_results = pd.read_csv(\"output/mysql_sqlite_results_after.csv\")\n",
    "reason_df = mysql_results[['SQL', 'EXPECTED_RESULT',\n",
    "                    'ACTUAL_RESULT', 'ERROR_REASON', 'IS_ERROR']]\n",
    "mysql_reasons = reason_df['ERROR_REASON'].value_counts()\n",
    "\n",
    "print(duckdb_reasons)\n",
    "print(postgresql_reasons)\n",
    "# outer join two dataframes\n",
    "\n",
    "join_df = pd.concat([duckdb_reasons, postgresql_reasons, mysql_reasons], axis=1, join='outer')\n",
    "# print(join_df)\n",
    "\n",
    "join_df.fillna(0, inplace=True)\n",
    "join_df = join_df.astype(int)\n",
    "\n",
    "# pure_reasons = reasons[['TAG', 'REASON']].drop_duplicates()\n",
    "pure_reasons = reasons[['TAG', 'SUPER_TAG']].drop_duplicates()\n",
    "print(pure_reasons['SUPER_TAG'].value_counts())\n",
    "# join the join_df and the reasons dataframe, drop duplicate rows\n",
    "join_df = join_df.join(pure_reasons.set_index('TAG'),  how='left')\n",
    "join_df.sort_values(by=['SUPER_TAG'], inplace=True)\n",
    "\n",
    "detail_df = join_df.copy()\n",
    "\n",
    "detail_df.drop(columns=['ERROR_REASON'], inplace=True)\n",
    "# sum the rows with the same SUPER_TAG\n",
    "join_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# unique_reasons = join_df['SUPER_TAG'].drop_duplicates()\n",
    "join_df = join_df.groupby('SUPER_TAG').sum()\n",
    "join_df.to_latex(\"output/error_reasons_highlevel.tex\", )\n",
    "# join_df.style.to_latex(\"output/error_reasons.tex\", )\n",
    "\n",
    "\n",
    "output_df = detail_df.value_counts()\n",
    "\n",
    "# unique_reasons = join_df['SUPER_TAG'].drop_duplicates()\n",
    "# print(unique_reasons)\n",
    "schema = \"\\\\newcommand{\\\\SQLiteCompatibilityIssue%s}{%s}\\n\"\n",
    "\n",
    "with open(\"output/Variables.tex\", \"a\") as f:\n",
    "\n",
    "    f.write(schema % ('SQL',  output_df['Command']))\n",
    "    # f.write(schema % ('Function', output_df['UFunction']))\n",
    "    f.write(schema % ('Type', output_df['Type']))\n",
    "    f.write(schema % ('Operator', output_df['Operator']))\n",
    "    # f.write(schema % ('Configuration', output_df['Setting']))\n",
    "    f.write(schema % ('Semantic', output_df['Function'] +\n",
    "            output_df['IOperator']))\n",
    "    # f.write(schema % ('Misc', output_df['Misc']))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
